# -*- coding: utf-8 -*-
"""Main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11swvsy3koaPQ5NGNszaRGb0ht-Meo5lq

## **Using Gradio to wrap a text to text interface around GPT-2** ##
Check out the library on github and see the getting started page for more demos.

### Installs and Imports
"""

!pip install -q gradio

# Remove sentence-transformers (it conflicts with transformers 5.x)
!pip uninstall -y sentence-transformers

!pip install -q git+https://github.com/huggingface/transformers.git

import gradio as gr
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

"""### Loading the model and tokenizer
Note: You can also change `gpt` to `gpt-xl` for a much more powerful model!


"""

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained(
    "gpt2",
    pad_token_id=tokenizer.eos_token_id
)

"""### Creating Generative Function"""

def generate_text(inp):

    input_ids = tokenizer.encode(inp, return_tensors="pt")

    beam_output = model.generate(
        input_ids,
        max_length=120,
        num_beams=5,
        no_repeat_ngram_size=2,
        early_stopping=True
    )

    output = tokenizer.decode(
        beam_output[0],
        skip_special_tokens=True
    )

    return ".".join(output.split(".")[:-1]) + "."

"""## Creating Interface and launching!"""

demo = gr.Interface(
    fn=generate_text,
    inputs=gr.Textbox(label="Input Text", lines = 10),
    outputs=gr.Textbox(label="Generated Text", lines = 10),
    title="GPT-2 Text Generator",
    description="Generate text using GPT-2 (Transformers dev version)."
)

demo.launch()